{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkable Cities\n",
    "DS4A Empowerment Cohort 4 Team 27: Sola Agogu | Daniel Bernal | Omar Ibarra | Chanel Lee | Dami Salami"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Most American cities are designed for cars instead of people. With over 2 million people injured in car accidents every year and 14% of greenhouse gas emissions coming from cars, policy makers are considering if is time for the US to move away from car-focused\n",
    "culture. As more cities move toward walkability, we are closer to a solution for curbing car-centrism in the US. From prioritizing pedestrian safety to public transportation, we will explore the effects of people-first city design.\n",
    "Pedestrian-focused, walkable cities invest in the community. They not only reduce car use, they improve local economies, reduce obesity and diabetes rates, and more. We will investigate how walkable cities can contribute to a better quality of life for people, and compare walkable cities to car-centric cities that treat pedestrians as an\n",
    "afterthought.\n",
    "We will explore what makes a city walkable by considering pedestrian-centric designs, public transportation, and city ordinances. \n",
    "We will also compare walkable cities to car-centric cities on quality of life factors. We then aim to answer the following question: What quality of life factors are higher in walkable cities?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Impact\n",
    "This project is meant to encourage city planners and local governments to invest in walkability and implement pedestrian-centric city designs. We hypothesize that walkable cities lead to an improvement in the quality of life for its residents and increase the local economy. We hope to inspire residents to ask their local government to help shape the future of our countryâ€™s infrastructure and health by bringing walkability to their city.\n",
    "\n",
    "*****************************************************\n",
    "** Include research on any other studies on this and approaches of the solutions to the main question. Provide references!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "***** EDIT: A number of studies have been done by researchers and students world-wide about the possibility of improving sustainability by building more walkable cities including ****, ******** and ** (references). Urban planning articles like (reference) extol the common sense benefits of walkable cities on the environment. As with most other facets of life, human beings tend to care about topics when they understand the specific benefits to them.\n",
    "Our approach to demonstrating this included regression analysis, and modeling to determine if there are any differences in quality of life between walkable and non-walkable cities. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis & Computation\n",
    "\n",
    "### Datasets\n",
    " Several American Community Survey datasets were obtained with 2021 estimates for different types of variables. These were valuable because they contained information at the city level.\n",
    " - Demographic variables including age, sex, education (***REFERENCE***)\n",
    " - Health variables including population percentages with diabetes, high blood pressure, obesity (***REFERENCE***)\n",
    " - Other quality of life health variables including sleep, depression, crime, and how often members of the population go for medical checkups (***REFERENCE***)\n",
    " Walkability variables including cities' walk scores and bike scores were obtained by web scraping the *** site (*REFERENCE*)\n",
    "\n",
    "\n",
    " ### Data Cleaning and Wrangling\n",
    " Our challenge was not just figuring out how to gather sufficient data, but also figuring out how to eliminate some of the 100+ variables from the ACS and EPA data sources.\n",
    " Doing this while maintaining the integrity of our dataset required extensive data cleaning and wrangling:\n",
    " - Using the python pandas library, we dropped repeated variables, columns that included annotations alone, and variables that were considered irrelevant to investigating our hypothesis like the emissions from power plants and other industries. \n",
    " - We also standardized the column names to aid in the readability of the data (S0101_15E became public_transportation, for example). \n",
    " - Unfortunately, the data cleaning and wrangling process also included getting rid of information with the wrong granularity, like EPA vehicle emissions data which was only available down to the State level. \n",
    " \n",
    " This section was by far the most time-consuming and took several rounds of refining until we had an appropriate dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit Exploratory Data Analysis \n",
    "\n",
    "In carrying out an exploratory analysis of our data, we found encouraging signs that there were some correlations between our walkability variables and our quality of life variables. \n",
    "\n",
    "******Below is a geographic bubble plot of city walkability vs ** (or other bivariate/multivariate visualization)\n",
    "******And below is our standardized data using SciKit Learn's StandardScaler \n",
    "\n",
    "\n",
    "include most meaningful data visualizations AND derived conclusions/remarkable hypothesis - use submitted extended analysis\n",
    "B\n",
    "\n",
    "\n",
    "#### 1. Data review\n",
    "#### 2. Summary Statistics\n",
    "#### 3. Scatterplots and other univariate distributions\n",
    "#### 4. Heat Maps (Correlations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Analysis & Predictive Modeling\n",
    "\n",
    "5. Hypothesis Testing\n",
    "focus on why twe made the choices we made\n",
    "6. Regression Analysis/ Modeling\n",
    "model implications and validity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of Dashboard\n",
    "\n",
    "### Use Case\n",
    "\n",
    "### Data Engineering\n",
    "\n",
    "### Flow charts/diagrams to show "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Future Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "import statsmodels.formula.api  as smf\n",
    "#import hvplot.pandas\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pingouin                 as pg\n",
    "import statsmodels.formula.api  as sm\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.io import output_notebook\n",
    "import cufflinks as cf\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasources/walkable-cities.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# import wrangled dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m walkable_cities \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mdatasources/walkable-cities.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m walkable_cities\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasources/walkable-cities.csv'"
     ]
    }
   ],
   "source": [
    "# import wrangled dataset\n",
    "walkable_cities = pd.read_csv(\"datasources/walkable-cities.csv\")\n",
    "walkable_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviewing a list of the dataset's columns\n",
    "display(walkable_cities.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our visual review of the dataset and columns, we can see that our variables include:\n",
    " - walkability variables: walk_score and bike_score\n",
    " - race variables\n",
    " - health variables\n",
    " - age variables\n",
    " - crime variables\n",
    " - commuting variables\n",
    " - age variable\n",
    " - gender variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REVIEW\n",
    "#Additional data wrangling\n",
    "#uses the loc method to select the subset of rows and columns where the column names are not duplicated. \n",
    "walkable_cities = walkable_cities.loc[:,~walkable_cities.columns.duplicated()]\n",
    "#removes the specified columns from the walkable_cities dataframe\n",
    "walkable_cities.drop(walkable_cities.columns[[4,5,6,7,11]], axis=1, inplace=True)\n",
    "walkable_cities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our main walkability variable, the frequency distribution in the histogram below shows a slight right skewness. We can explore it further by carrying out descriptive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_walk = walkable_cities['walk_score']\n",
    "\n",
    "# Plot a histogram of the \"walk_score\" column\n",
    "sns.displot(hist_walk, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For review purposes, group the walk scores by state\n",
    "pd.options.plotting.backend='hvplot'\n",
    "walkable_cities.groupby('state')['walk_score'].mean().plot(kind='bar', width=1400, height=600, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the city with highest walk score\n",
    "pd.options.plotting.backend='hvplot'\n",
    "walkable_cities.groupby('city')['walk_score'].mean().plot(kind='bar', bins=100, width=1600, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New dataframe with walk score greater than 50\n",
    "poss_walK_cities = walkable_cities.groupby('city').filter(lambda x: x['walk_score'].max() > 50)\n",
    "poss_walK_cities.sort_values('walk_score')\n",
    "poss_walK_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify only the cities with walk score over 70\n",
    "poss_walk_sort = poss_walK_cities.groupby('city').mean().sort_values('walk_score')\n",
    "#Other way: poss_walk_sort.groupby(\"city\" ).apply(lambda x: x.sort_values(\"walk_score\"))\n",
    "posswalk_over = poss_walk_sort[['walk_score']].sort_values(by='walk_score', ascending=False)\n",
    "posswalk_over.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walkable_cities.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# define variables\n",
    "\"\"\"\n",
    "x = np.array(walkable_cities[['pop_per_km2', 'median_age', 'male',\n",
    "       'female',\n",
    "       'access2', 'arthritis', 'binge', 'bphigh', 'bpmed', 'cancer', 'casthma',\n",
    "       'cervical', 'chd', 'checkup', 'cholscreen', 'colon_screen', 'copd',\n",
    "       'corem', 'corew', 'csmoking', 'dental', 'depression', 'diabetes',\n",
    "       'ghlth', 'highchol', 'kidney', 'lpa', 'mammouse', 'mhlth', 'obesity',\n",
    "       'phlth', 'sleep', 'stroke', 'teethlost', 'cumulative_confirmed',\n",
    "       'cumulative_deceased', 'drive_commute',\n",
    "       'public_transit_commute', 'walk_commute', 'bike_commute',\n",
    "       'work_from_home', 'households', 'mean_household_income', 'mean_income',\n",
    "       'median_household_income', 'living_wage', 'poverty',\n",
    "       'unemployment_rate', 'median_aqi', 'violent_crime', 'property_crime']])\n",
    "\"\"\"\n",
    "x = walkable_cities[['pop_per_km2', 'median_age', 'male',\n",
    "       'female',\n",
    "       'access2', 'arthritis', 'binge', 'bphigh', 'bpmed', 'cancer', 'casthma',\n",
    "       'cervical', 'chd', 'checkup', 'cholscreen', 'colon_screen', 'copd',\n",
    "       'corem', 'corew', 'csmoking', 'dental', 'depression', 'diabetes',\n",
    "       'ghlth', 'highchol', 'kidney', 'lpa', 'mammouse', 'mhlth', 'obesity',\n",
    "       'phlth', 'sleep', 'stroke', 'teethlost', 'cumulative_confirmed',\n",
    "       'cumulative_deceased', 'drive_commute',\n",
    "       'public_transit_commute', 'walk_commute', 'bike_commute',\n",
    "       'work_from_home', 'households', 'mean_household_income', 'mean_income',\n",
    "       'median_household_income', 'living_wage', 'poverty',\n",
    "       'unemployment_rate', 'median_aqi', 'violent_crime', 'property_crime']].values\n",
    "\n",
    "y = np.array(walkable_cities['walk_score'])\n",
    "\n",
    "# create a list of all possible variable combinations\n",
    "combinations_list = list(combinations(['pop_per_km2', 'median_age', 'male',\n",
    "       'female',\n",
    "       'access2', 'arthritis', 'binge', 'bphigh', 'bpmed', 'cancer', 'casthma',\n",
    "       'cervical', 'chd', 'checkup', 'cholscreen', 'colon_screen', 'copd',\n",
    "       'corem', 'corew', 'csmoking', 'dental', 'depression', 'diabetes',\n",
    "       'ghlth', 'highchol', 'kidney', 'lpa', 'mammouse', 'mhlth', 'obesity',\n",
    "       'phlth', 'sleep', 'stroke', 'teethlost', 'cumulative_confirmed',\n",
    "       'cumulative_deceased', 'drive_commute',\n",
    "       'public_transit_commute', 'walk_commute', 'bike_commute',\n",
    "       'work_from_home', 'households', 'mean_household_income', 'mean_income',\n",
    "       'median_household_income', 'living_wage', 'poverty',\n",
    "       'unemployment_rate', 'median_aqi', 'violent_crime', 'property_crime'], 2))\n",
    "\n",
    "# loop through the list of combinations\n",
    "for combination in combinations_list:\n",
    "    # select the variables for this iteration\n",
    "    var_indices = [np.where(x == var)[1][0] for var in combination]\n",
    "    x_temp = x[:, [np.where(x.columns == var)[0][0] for var in combination]]\n",
    "\n",
    "\n",
    "    \n",
    "    # fit the model\n",
    "    model = LinearRegression().fit(x_temp, y)\n",
    "    \n",
    "    # calculate the R-squared\n",
    "    r2 = r2_score(y, model.predict(x_temp))\n",
    "    \n",
    "    # print the combination and the R-squared\n",
    "    print(combination, r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the cities with walk score greater than 50\n",
    "pd.options.plotting.backend='hvplot'\n",
    "poss_walK_cities.groupby('city')['walk_score'].mean().plot(kind='bar', width=1400, height=600, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify only the cities with walk score less 70\n",
    "posswalk_over = poss_walk_sort[['walk_score']].sort_values(by='walk_score', ascending=True)\n",
    "posswalk_over.head(144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New dataframe with walk score greater less than 50\n",
    "no_walK_cities = walkable_cities.groupby('city').filter(lambda x: x['walk_score'].max() <= 50)\n",
    "no_walK_cities.sort_values('walk_score')\n",
    "no_walK_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the cities with walk score less than 50\n",
    "pd.options.plotting.backend='hvplot'\n",
    "no_walK_cities.groupby('city')['walk_score'].mean().plot(kind='bar', width=1600, height=600, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum number of rows and columns displayed\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "# Identify all NA values in the dataframe\n",
    "na_values = walkable_cities.isna().sum()\n",
    "\n",
    "# Print the resulting dataframe\n",
    "display(na_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary Statistics Table\n",
    "walkable_cities.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot matrix\n",
    "sns.pairplot(walkable_cities[['walk_score', 'bike_score', 'chd', 'obesity', 'bphigh', 'highchol', 'diabetes']])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a scatter plot matrix\n",
    "scatter_matrix = pd.plotting.scatter_matrix(walkable_cities, figsize=(20, 20))\n",
    "\n",
    "# Rotate the x-axis labels on the scatter plot matrix\n",
    "[s.xaxis.label.set_rotation(45) for s in scatter_matrix.reshape(-1)]\n",
    "[s.yaxis.label.set_rotation(0) for s in scatter_matrix.reshape(-1)]\n",
    "[s.get_yaxis().set_label_coords(-0.3,0.5) for s in scatter_matrix.reshape(-1)]\n",
    "\n",
    "# Set the labels for the x-axis and y-axis\n",
    "[s.xaxis.set_label_text(col) for col, s in zip(walkable_cities.columns, scatter_matrix.reshape(-1))]\n",
    "[s.yaxis.set_label_text(col) for col, s in zip(walkable_cities.columns, scatter_matrix.reshape(-1))]\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heat Maps\n",
    "# Create a list only with the features needed\n",
    "walkable_values = ['walk_score', 'bike_score', 'chd', 'obesity', 'bphigh', 'highchol', 'diabetes']\n",
    "# Create a pivot table with the \"city\" column as the index and the \"walk_score\" column as the values\n",
    "walkable_pivot_table = walkable_cities.pivot_table(index=\"city\", values=walkable_values)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 50))\n",
    "# Create a heatmap of the pivot table\n",
    "sns.heatmap(walkable_pivot_table, cmap=\"coolwarm\", ax=ax)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group the cities in a dataframe by the \"walk_score\" column and create a heat map\n",
    "\n",
    "# Create a new column with the \"walk_score\" values binned into ranges of 10\n",
    "walkable_cities[\"walk_score_range\"] = pd.cut(walkable_cities[\"walk_score\"], bins=range(0, 110, 10))\n",
    "\n",
    "# Create a pivot table with the \"city\" column as the index and the \"walk_score_range\" column as the values\n",
    "walkable_pivot_table_1  = walkable_cities.pivot_table(index=\"city\", values=\"walk_score_range\")\n",
    "\n",
    "# Create a heatmap of the pivot table\n",
    "sns.heatmap(walkable_pivot_table_1)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the pivot table contains any non-NaN values\n",
    "if walkable_pivot_table_1.notnull().values.any():\n",
    "    # Create a heatmap of the pivot table\n",
    "    sns.heatmap(walkable_pivot_table_1)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"The pivot table does not contain any non-NaN values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations\n",
    "\n",
    "top_neg_corr_walk = walkable_cities.corr()['walk_score'].sort_values(ascending = True)[1:11]\n",
    "top_pos_corr_walk = walkable_cities.corr()['walk_score'].sort_values(ascending = False)[1:11]\n",
    "top_neg_corr_bike = walkable_cities.corr()['walk_score'].sort_values(ascending = True)[1:11]\n",
    "top_pos_corr_bike = walkable_cities.corr()['walk_score'].sort_values(ascending = False)[1:11]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top possitive correlations\n",
    "top_pos_corr_walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top negative correlations\n",
    "top_neg_corr_walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis Testing and model\n",
    "\n",
    "# Create a multiple linear regression model with the dependent variable and the independent variables\n",
    "model = smf.ols('walk_score ~ bike_score + chd + obesity + bphigh + highchol + diabetes', data=walkable_cities).fit()\n",
    "\n",
    "# Dependent variable - 'walk_score'\n",
    "# Independent variables - 'bike_score', 'chd', 'obesity', 'bphigh', 'highchol', 'diabetes'\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model.summary())\n",
    "\n",
    "# Define the null and alternate hypotheses\n",
    "null_hypothesis = \"There is no relationship between walk_score and the independent variables\"\n",
    "alternate_hypothesis = \"There is a relationship between walk_score and the independent variables\"\n",
    "\n",
    "# Perform the F-test\n",
    "f_test = model.f_test(np.identity(len(model.params)))\n",
    "\n",
    "# Print the p-value of the F-test\n",
    "print(f\"p-value = {f_test.pvalue:.4f}\")\n",
    "\n",
    "# Interpret the results\n",
    "if f_test.pvalue < 0.05:\n",
    "    print(\"Reject the null hypothesis\")\n",
    "    print(alternate_hypothesis)\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis\")\n",
    "    print(null_hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression model\n",
    "\n",
    "walkable_cities_1 = walkable_cities.loc[walkable_cities['walk_score'] >= 0, walkable_values]\n",
    "\n",
    "type(walkable_cities_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the dataframe into two groups\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "greater_than_50 = walkable_cities_1[walkable_cities_1['walk_score'] > 50]\n",
    "\n",
    "less_than_or_equal_to_50 = walkable_cities_1[walkable_cities_1['walk_score'] <= 50]\n",
    "\n",
    "# Define the features and target variable\n",
    "X = greater_than_50.drop(columns='walk_score')\n",
    "y = greater_than_50['walk_score']\n",
    "\n",
    "# check the shape of greater_than_50 dataframe\n",
    "print(greater_than_50.shape)\n",
    "\n",
    "# check if walk_score column exists in the dataframe\n",
    "print(greater_than_50.columns)\n",
    "\n",
    "#check if there are any missing value in the dataframe\n",
    "print(greater_than_50.isnull().sum())\n",
    "\n",
    "# check if all columns are numeric\n",
    "print(greater_than_50.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Drop missing values using dropna()\n",
    "X_train.dropna()\n",
    "X_test.dropna()\n",
    "\n",
    "# Check the shape of the X_train variable\n",
    "print(X_train.shape)\n",
    "\n",
    "# Check if X_train is empty\n",
    "if X_train.shape[0] == 0:\n",
    "    print(\"X_train is empty.\")\n",
    "\n",
    "# Print the first 5 rows of X_train\n",
    "print(X_train.head())\n",
    "\n",
    "print(X_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create an imputer with mean strategy\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit and transform the imputer on the training set\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test set\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Scale and normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Initialize the model\n",
    "reg = LinearRegression()\n",
    "\n",
    "# Train the model on the training set\n",
    "reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = reg.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the mean squared error and R^2 score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error: {:.2f}\".format(mse))\n",
    "print(\"R-squared value: {:.2f}\".format(r2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c6dd11d78fc0321aac89a6f4a5685aaa94ef29d42e26e02187d83b05ac9a6e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
