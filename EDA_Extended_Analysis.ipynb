{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Identify all NAs in the data.\n",
    "2. Summary Statistics Table\n",
    "3. Scatterplots\n",
    "4. Heat Maps (Correlations)\n",
    "5. Hypothesis Testing\n",
    "6. Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "walking_data = pd.read_csv(\"data_sources/walkable-cities.csv\",encoding='latin-1')\n",
    "# walking_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Identify all NAs in the data.\n",
    "\n",
    "col_na = walking_data.isna().sum().to_frame().reset_index(level=0).set_axis([\"variable\",\"sum_na\"], axis=\"columns\", copy=False)\n",
    "row_na = walking_data[\"city_state\"]\n",
    "row_na = pd.concat([row_na, walking_data.isna( ).sum(axis=\"columns\")], axis=1)\n",
    "row_na = row_na.set_axis([\"city_state\",\"sum_na\"],axis=\"columns\", copy=False)\n",
    "\n",
    "col_na.to_csv(\"column_na_orig.csv\", index = False)\n",
    "row_na.to_csv(\"row_na_orig.csv\", index = False)\n",
    "\n",
    "# remove cols and rows with more than 15% NAs\n",
    "nrow = len(walking_data)\n",
    "\n",
    "keep_cols = list(walking_data.columns[col_na[\"sum_na\"]/nrow <= .15])\n",
    "walking_data = walking_data[keep_cols]\n",
    "\n",
    "mcol = len(walking_data.columns)\n",
    "\n",
    "row_na = walking_data[\"city_state\"]\n",
    "row_na = pd.concat([row_na, walking_data.isna( ).sum(axis=\"columns\")], axis=1)\n",
    "row_na = row_na.set_axis([\"city_state\",\"sum_na\"],axis=\"columns\", copy=False)\n",
    "\n",
    "keep_rows = row_na[\"sum_na\"]/mcol <= .15\n",
    "walking_data = walking_data[keep_rows]\n",
    "\n",
    "walking_data #725 rows Ã— 66 columns\n",
    "\n",
    "#calc NAs again\n",
    "col_na = walking_data.isna().sum().to_frame().reset_index(level=0).set_axis([\"variable\",\"sum_na\"], axis=\"columns\", copy=False)\n",
    "row_na = walking_data[\"city_state\"]\n",
    "row_na = pd.concat([row_na, walking_data.isna( ).sum(axis=\"columns\")], axis=1)\n",
    "row_na = row_na.set_axis([\"city_state\",\"sum_na\"],axis=\"columns\", copy=False)\n",
    "\n",
    "col_na.to_csv(\"column_na_after.csv\", index = False)\n",
    "row_na.to_csv(\"row_na_after.csv\", index = False)\n",
    "\n",
    "walking_data.to_csv(\"walkable_cities.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "demographic = [\"pop_estimate_2021\", \"land_area_sqkm\", \n",
    "                \"pop_per_km2\", \"white_alone\", \"black_or_african_american_alone\", \n",
    "                \"american_indian_and_alaska_native_alone\", \"asian_alone\", \n",
    "                \"native_hawaiian_and_other_pacific_islander_alone\", \n",
    "                \"some_other_race_alone\", \"two_or_more_races\", \"hispanic_or_latino\"]\n",
    "environment = [\"median_aqi\"]\n",
    "health = ['access2', 'arthritis', 'binge', 'bphigh', 'bpmed', 'cancer', 'casthma',\n",
    "       'cervical', 'chd', 'checkup', 'cholscreen', 'colon_screen', 'copd',\n",
    "       'corem', 'corew', 'csmoking', 'dental', 'depression', 'diabetes',\n",
    "       'ghlth', 'highchol', 'kidney', 'lpa', 'mammouse', 'mhlth', 'obesity',\n",
    "       'phlth', 'sleep', 'stroke', 'teethlost', 'cumulative_confirmed',\n",
    "       'cumulative_deceased']\n",
    "name = [\"place_code\",\n",
    "        \"city\", \n",
    "        \"state\", \n",
    "        \"state_code\", \n",
    "        \"city_state\", \n",
    "        \"city_state_code\", \n",
    "        \"place_state_code\", \n",
    "        \"county_state\",\n",
    "        \"geo_code\"]\n",
    "walkability = [\"walkable\",\n",
    "        \"walk_score\",\n",
    "        \"bike_score\"]\n",
    "wealth = [\"living_wage\"]\n",
    "\n",
    "response = [\"walk_score\"]\n",
    "\n",
    "feature = demographic + environment + health + wealth\n",
    "# feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Summary Statistics Table\n",
    "import xlwt\n",
    "from xlwt import Workbook\n",
    "walkable_cities = walking_data[walking_data[\"walkable\"]==\"walkable\"]\n",
    "car_cities = walking_data[walking_data[\"walkable\"]==\"car dependent\"]\n",
    "\n",
    "with pd.ExcelWriter(\"Summary Statistics.xlsx\") as writer: \n",
    "        walking_data[response+feature].describe().to_excel(writer, sheet_name=\"All Cities\")\n",
    "        walkable_cities[response+feature].describe().to_excel(writer, sheet_name= \"Walkable Cities\")\n",
    "        car_cities[response+feature].describe().to_excel(writer, sheet_name= \"Car Dependent Cities\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Scatterplots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "walk_score               1.000000\n",
       "pop_per_km2              0.712861\n",
       "living_wage              0.457959\n",
       "some_other_race_alone    0.388525\n",
       "arthritis               -0.301249\n",
       "csmoking                -0.301729\n",
       "highchol                -0.313908\n",
       "white_alone             -0.318448\n",
       "depression              -0.333355\n",
       "bphigh                  -0.340854\n",
       "obesity                 -0.364233\n",
       "Name: walk_score, dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Heat Maps (Correlations)\n",
    "my_cor = walking_data[response+feature].corr()['walk_score'].sort_values(ascending=False)\n",
    "my_cor.to_csv(\"correlations.csv\", index = False)\n",
    "correlated = abs(my_cor) > .30\n",
    "my_cor[correlated] # moderate to strong correlations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Hypothesis Testing\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import pingouin as pg\n",
    "import csv\n",
    "\n",
    "final_result = []\n",
    "# Conducting two-sample ttest\n",
    "for feat in feature:\n",
    "    result = pg.ttest(walkable_cities[feat],\n",
    "                    car_cities[feat],\n",
    "                    correction=True)\n",
    "\n",
    "    final_result.append([feat, result])\n",
    "\n",
    "with open('ttests.csv', 'w') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        for row in final_result:\n",
    "            csvwriter.writerow(row)   \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def interp(data):\n",
    "\n",
    "    # Do the original interpolation\n",
    "    data.interpolate(method='nearest', xis=0, inplace=True)\n",
    "\n",
    "    # # Display result\n",
    "    # print ('Interpolated data:')\n",
    "    # print (data)\n",
    "    # print ()\n",
    "\n",
    "    # Function to curve fit to the data\n",
    "    def func(x, *params):\n",
    "        y = 0\n",
    "        for i in range(len(params)):\n",
    "            y += params[i] * (x ** i)\n",
    "        return y\n",
    "\n",
    "    # Initial parameter guess, just to kick off the optimization\n",
    "    guess = [1] * len(data.columns)\n",
    "\n",
    "    # Create copy of data to remove NaNs for curve fitting\n",
    "    fit_data = data.dropna()\n",
    "\n",
    "    # Place to store function parameters for each column\n",
    "    col_params = {}\n",
    "\n",
    "    # Curve fit each column\n",
    "    for col in fit_data.columns:\n",
    "        # Get x & y\n",
    "        x = fit_data.index.astype(float).values\n",
    "        y = fit_data[col].values\n",
    "        # Curve fit column and get curve parameters\n",
    "        params = curve_fit(func, x, y, guess)\n",
    "        # Store optimized parameters\n",
    "        col_params[col] = params[0]\n",
    "\n",
    "    # Extrapolate each column\n",
    "    for col in data.columns:\n",
    "        # Get the index values for NaNs in the column\n",
    "        x = data[pd.isnull(data[col])].index.astype(float).values\n",
    "        # Extrapolate those points with the fitted function\n",
    "        data[col][x] = func(x, *col_params[col])\n",
    "\n",
    "    # # Display result\n",
    "    # print ('Extrapolated data:')\n",
    "    # print (data)\n",
    "    # print ()\n",
    "\n",
    "    # print ('Data was extrapolated with these column functions:')\n",
    "\n",
    "    # # Iterate through the number of parameters in the col_params dictionary and add the corresponding term \n",
    "    # # for each parameter in the string that is being printed.\n",
    "    # for col in col_params:\n",
    "    #     terms = []\n",
    "    #     for i in range(len(col_params[col])):\n",
    "    #         terms.append(\"{:0.3e} x^{}\".format(col_params[col][i], i))\n",
    "    #     print(\"f_{}(x) = {}\".format(col, \" + \".join(terms)))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chane\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\optimize\\_minpack_py.py:881: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "C:\\Users\\chane\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\optimize\\_minpack_py.py:881: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = walking_data[feature]\n",
    "y = walking_data[response]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train = pd.DataFrame(X_train).set_axis(feature, axis='columns')\n",
    "X_test = pd.DataFrame(X_test).set_axis(feature, axis='columns')\n",
    "y_train = pd.DataFrame(y_train).set_axis(response, axis='columns')\n",
    "y_train = pd.DataFrame(y_train).set_axis(response, axis='columns')\n",
    "\n",
    "X_train = interp(X_train)\n",
    "X_test = interp(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA - Principal component analysis\n",
    "\n",
    "# Unsupervised Machine Learning Algorithm for Dimensionality Reduction\n",
    "\n",
    "# normalize the data\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Create an imputer object with strategy='mean' or 'median'\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit and transform the data using the imputer\n",
    "data_scaled = imputer.fit_transform(data_scaled)\n",
    "\n",
    "#Initialize the PCA model and specify the number of components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the PCA model to your data\n",
    "pca.fit(data_scaled)\n",
    "\n",
    "# Apply the dimensionality reduction to your data\n",
    "data_reduced = pca.transform(data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principal Component 1:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feature_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPrincipal Component \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m     \u001b[39mfor\u001b[39;00m j, feature_coefficient \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(component):\n\u001b[1;32m---> 30\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfeature_names[j]\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mfeature_coefficient\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[39m# explained_variance = pca.explained_variance_ratio_\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \n\u001b[0;32m     35\u001b[0m \u001b[39m# classifier = LogisticRegression(random_state = 0)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39m# classifier.fit(X_train, y_train)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[39m# cm = confusion_matrix(y_test, y_pred)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_names' is not defined"
     ]
    }
   ],
   "source": [
    "# 6. Regression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "  \n",
    "# Fit the PCA model to your data\n",
    "pca.fit(X_train)\n",
    "\n",
    "# Apply the dimensionality reduction to your data\n",
    "data_reduced = pca.transform(X_train)\n",
    "\n",
    "data_reduced\n",
    "\n",
    "components = pca.components_\n",
    "\n",
    "# Print the names of the features for each principal component\n",
    "for i, component in enumerate(components):\n",
    "    print(f\"Principal Component {i+1}:\")\n",
    "    for j, feature_coefficient in enumerate(component):\n",
    "        print(f\"{feature_names[j]}: {feature_coefficient}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "# explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# classifier = LogisticRegression(random_state = 0)\n",
    "# classifier.fit(X_train, y_train)\n",
    "\n",
    "# cm = confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97c27d02179f6ac452fd1884d10c0b371bca7567d38c61431769d872f52b3e24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
